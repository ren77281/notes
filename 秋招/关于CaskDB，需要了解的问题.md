```toc
```
## 为什么要做这个项目？
因为学习了15445这门公开课，对数据库的存储模块比较感兴，就去了解了更多的底层数据结构，比如关系型数据库的B+树，可扩展哈希表。kv数据库的lsm tree, 跳表, bitcask。想着深入了解这些数据结构的区别，就以bitcask作为kv数据库的存储模型，复现了论文中的bitcask。这个过程中遇到了许多难点，比如数据库的备份如何实现，merge清理无用数据如何实现。这个过程中也能接触很多存储知识，通过学习这些知识不断地优化自己的项目，提升它的性能表现，是一件正反馈很强的事。
## BitCask介绍
bitcask 是一种基于日志，具有持久化特性的键值存储引擎。一个bitcask实例有多个数据文件，其中只有一个活跃数据文件。所有的修改操作实际上是对活跃数据文件的追加，追加完成后将更新内存索引。内存索引保存了键值对在磁盘上的位置，读取数据时，先根据索引以及Key查找出数据在磁盘上的位置，然后再从磁盘中取出实际的数据。

bitcask的读写性能十分高效，因为写入操作的开销只有一次磁盘I/O，更新内存中的索引带来的开销可以忽略不记。而读取操作的开销也是一次内存访问与磁盘I/O。
## 内存索引结构设计
使用了ART树、B树、B+树作为索引结构，用户启动bitcask时，可以选择合适的结构作为内存索引。其中ART树、B树面向内存存储，能存储的数据量受到了内存大小的限制，且保存的数据不具有持久性。而B+树面向磁盘存储，将索引信息持久化到磁盘中，能存储的数据量突破了内存大小的限制。

具体实现中，我定义了一个Indexer接口，声明了Put，Get，Delete这些基础方法。具体的索引结构只实现并封装这些接口即可。Indexer通过NewIterator()函数与外部交互。
## 磁盘调度器设计
用户可以选择以普通文件或者MMap的方式读写磁盘，因此这里定义了IOManager接口，声明了Read，Write，Sync，Close等方法。底层的磁盘读写方法只需要实现并封装这些接口即可，IOManager通过NewIOManager函数与外部交互。

数据库将通过一个名为DataFile的类完成数据的读写，那么DataFile实际上封装了IOManager，文件ID以及存储的数据量。
## 写流程
数据格式很简单，包括了元数据与实际的数据。原数据描述了key，value的大小，以及一个校验值。实际的数据就是key，value以及删除标志位。写入数据前，需要检查是否存在活跃文件，以及活跃文件是否达到阈值。如果不存在活跃文件，说明这是第一次启动，需要创建并初始化活跃文件。如果已存在的活跃文件的数据量达到阈值，则需要关闭它，再创建新的活跃文件。总之，bitcask只有一个活跃文件用于写入。写入时将调用IOManager的Write接口，最后将返回数据在磁盘上的位置信息（文件ID+偏移量+大小）。最后将位置信息与key值将被更新到内存索引中。

删除数据则是向数据文件追加一个被标记为删除的数据，只是在追加后需要删除内存中的索引信息，而不是添加信息。
## 读流程
读取数据时，将根据数据的key值，通过内存索引查找数据在磁盘中的位置（文件ID+偏移量+大小）。根据文件ID获取数据文件并通过偏移量与大小，获取实际的数据。也就是先访问内存再进行一次磁盘IO。
## 数据库加载流程
和数据库实例有关的所有信息都封装在名为DB的结构体中，DB存储用户的配置选项，内存中的索引结构，指向活跃文件的指针以及存储所有数据文件的map表。数据库启动时将加载配置选项中的数据目录下的所有数据文件，这些数据文件的名字是一个从0开始递增的数列，最大的数据文件将被指定为活跃文件。

我们需要根据从旧到新的顺序遍历数据文件，也就是根据文件ID从小到大的顺序遍历，并构造索引信息。对于每个文件，需要维护offset变量，也就是每条数据在文件中的偏移值。对于每个数据，我们可以先获取其固定大小的元数据，再通过元数据获取实际的数据，并统计整体的大小，然后维护offset，并加载索引信息（文件ID+偏移量+大小）。
## ART介绍
ART是一种自适应基数树，它结合了前缀树和压缩存储的特点，适用于高效的键值存储。ART树的核心思想是通过动态调整节点大小，以减少内存占用和提高查找效率。节点可以有不同的结构，根据子节点数量动态选择合适的节点类型，如4、16、48或256个子节点，从而在稀疏和密集节点之间取得平衡。ART树将前缀相同的路径压缩成一个节点，减少树的深度，避免冗余的空间浪费，提高查询效率。查找时间复杂度接近于B树的O(log n)，但由于节点压缩，实际性能更接近O(1)，尤其适合内存访问。ART树适合处理具有大量前缀重叠的键，如字符串或IP地址查询。并能高效支持插入、删除和查找操作。
## 文件I/O与mmap
1. **实现机制**：文件I/O 通过系统调用 `read/write` 进行数据传输，涉及用户态与内核态之间的数据拷贝；`mmap` 则将文件直接映射到进程的虚拟内存空间，减少了拷贝开销。
2. **性能**：`mmap` 性能更高，尤其在大文件或随机读写场景下，避免了频繁的系统调用和数据拷贝，而文件I/O 则适合顺序读写。
3. **内存使用**：文件I/O 需要显式的缓冲区，`mmap` 则通过分页机制按需加载，节省物理内存。
4. **文件大小**：`mmap` 支持处理超大文件，利用虚拟内存机制，而文件I/O 受限于可用内存。
5. **一致性**：文件I/O 需要显式同步，`mmap` 自动同步到磁盘，但需注意崩溃情况下的数据丢失。

总结：`mmap` 适合大文件和高效读写场景，文件I/O 则更适合小文件和简单操作。
## WB实现
封装了一个WriteBatch结构体，使用WB功能时，所有对数据库的修改操作都将暂存到内存，也就是结构体中
## RESP支持
导入redcon框架，使用NewServer方法创建服务器，并封装listen，accept，close方法
```toc
```
malloc是语言提供的一个内存池，STL为频繁申请小块内存的操作也定制了一个内存池，而高并发内存池则是针对多线程场景下的优化
## 我的命名规则
类名：大驼峰
函数名与变量名：下划线
类成员以下划线开头

## 定长内存池的实现
为什么要实现定长内存池？原因是为了追求极致的效率，定长内存池不考虑内存碎片的问题，即假设用户每次申请的内存长度固定。

与STL的空间配置器对比，为了追求极致的效率与减少内存碎片的产生，STL空间配置器的内存池不是定长的，用户可以申请的内存长度为8B、16B...、128B，总之就是8的倍数，申请这些不同大小的内存块很容易导致内存碎片的问题，所以空间配置器不仅要保证极致的效率还有防止内存碎片的问题。这也是空间配置器和定长内存池的自由链表类型不同的缘由。空间配置器的自由链表是一个哈希桶，存储的是二级指针。而定长内存池的自由链表是一个单链表，存储的是一级指针。所以定长内存池的实现更为简单。

### 定长内存池的结构
与空间配置器一样，定长内存池用两个指针start和finish维护内存池。若只使用start指针，则无法确定内存池的使用情况：是否使用完了？还剩多少内存没有使用？这将导致内存池耗尽而我们却不知情，进而导致内存的越界访问。由于内存池是连续的，用户申请与归还内存块的顺序大概率是不同的，所以我们无法只用内存池维护用户归还的内存块。使用自由链表维护用户归还的内存块，当用户申请内存块时，优先使用自由链表中的内存块。因为从内存池拿走的内存块被归还到自由链表中了。因此除了一个内存池，我们还需要一个自由链表维护内存块的申请与归还。

### 定长内存池的内存操作
定长内存池主要的接口有两个：new_obj和delete_obj，分别负责分配内存块给用户与归还用户的内存块。

- new_obj：优先检查自由链表是否为空
  - 若不为空，则将链表的第一个内存块返回（*头删*）
  - 若为空，则填充内存池，并截取一块内存块返回
- delete_obj：将用户归还的内存块头插到自由链表

在这两个接口中用到了一个操作：对void\*\*的解引用。我们知道32位系统与64位系统的指针大小是不相同的。在自由链表中，内存块需要用4/8字节的空间存储下一内存块的地址，类似单链表中的next指针。一种简单的解决方式是：使用**条件编译**判断当前系统的位数，从而确定指针的字节数。另一种方法是使用\*(void\*\*)，void\*作为一个指针变量，在32位系统下大小为4字节，在64位系统下大小为8字节。将内存块的地址强转成void\*，再通过解引用void\*的地址访问void\*，这个操作在32位系统下可以访问4字节空间，64位系统下可以访问8字节空间，很好的规避了条件编译的繁琐。当然了，\*(void\*\*)中的void可以被替换成任意类型。（*ps：SGI版本的STL中，使用了union联合体代替这种强转操作，这也是一种不错的解决方法*）

以下是定长内存池的所有实现：
```cpp
#pragma once

#include "Common.hpp"

#ifdef _WIN32
	#include <Windows.h>
#else
// 其他系统的头文件
#endif

static void* SystemAlloc(size_t kpage)
{
#ifdef _WIN32
	void* p = VirtualAlloc(0, kpage << 12, MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);

#else
	// 其他系统申请内存的函数
#endif

	if (p == nullptr)
		throw std::bad_alloc();

	return p;
}


template <class T>
class ObjectPool
{
private:
	char* _start_free = nullptr;
	char* _finish_free = nullptr;     // 维护内存池的两个指针
	void* _free_list = nullptr;  // 自由链表
public:
	// 内存块的申请与归还
	T* new_obj();
	void delete_obj(T* obj);
};

template <class T>
T* ObjectPool<T>::new_obj()
{
	T* obj = nullptr;

	// 优先向自由链表申请内存块
	if (_free_list)
	{
		obj = (T*)_free_list;
		_free_list = *(void**)_free_list;
	}
	// 自由链表无内存块，向内存池索要
	else
	{
		// 内存池空间不足
		if ((_finish_free - _start_free) < sizeof(T))
		{
			size_t bytes_to_get = 128 * 1024; // 默认每次申请128kB空间
			_start_free = (char*)SystemAlloc(bytes_to_get >> 12);
			// _start_free = (char*)malloc(bytes_to_get);
			if (_start_free == nullptr)
			{
				throw std::bad_alloc();
			}

			_finish_free = _start_free + bytes_to_get;
		}

		size_t need_bytes = sizeof(T) > sizeof(void*) ? sizeof(T) : sizeof(void*);
		obj = (T*)_start_free;
		_start_free += need_bytes;
	}

	// 定位new，调用T的构造函数以进行初始化
	new(obj)T;

	return obj;
}

template <class T>
void ObjectPool<T>::delete_obj(T* obj)
{
	obj->~T();

	*(void**)obj = _free_list;
	_free_list = (void*)obj;
}
```

其中对于申请内存的操作使用了系统调用，不再使用malloc。Windows下malloc封装了VirtualAlloc，Linux下malloc封装了brk，利用条件编译使不同的系统调用相应的系统调用，使我们的定长内存池完全脱离malloc。并且，定长内存池的new_obj和delete_obj操作不仅要分配空间，还要进行初始化与销毁工作，所以new_obj的最后需要使用定位new，delete_obj一开始要调用对象的析构。

最后进行效率的测试，测试demo：
```cpp
struct TreeNode
{
	int _val;
	TreeNode* _left;
	TreeNode* _right;

	TreeNode()
		:_val(0)
		, _left(nullptr)
		, _right(nullptr)
	{}
};

void TestObjectPool()
{
	// 申请释放的轮次
	const size_t Rounds = 5;

	// 每轮申请释放多少次
	const size_t N = 100000;

	std::vector<TreeNode*> v1;
	v1.reserve(N);

	size_t begin1 = clock();
	for (size_t j = 0; j < Rounds; ++j)
	{
		for (int i = 0; i < N; ++i)
		{
			v1.push_back(new TreeNode);
		}
		for (int i = 0; i < N; ++i)
		{
			delete v1[i];
		}
		v1.clear();
	}

	size_t end1 = clock();

	std::vector<TreeNode*> v2;
	v2.reserve(N);

	ObjectPool<TreeNode> TNPool;
	size_t begin2 = clock();
	for (size_t j = 0; j < Rounds; ++j)
	{
		for (int i = 0; i < N; ++i)
		{
			v2.push_back(TNPool.new_obj());
		}
		for (int i = 0; i < N; ++i)
		{
			TNPool.delete_obj(v2[i]);
		}
		v2.clear();
	}
	size_t end2 = clock();

	std::cout << "测试数量n = " << N << std::endl;
	std::cout << "使用new花费的时间:" << end1 - begin1 << std::endl;
	std::cout << "使用定长内存池花费的时间:" << end2 - begin2 << std::endl;
}
```
测试结果：
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230430104752.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230430104709.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230430104820.png)

## 高并发内存池的整体框架
![](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230429152308.png)

一般内存池都需要考虑两个问题：
1. 内存碎片问题
2. 性能问题

高并发内存池还需要考虑：线程加锁导致的竞争问题。高并发内存池使用三层结构以解决这三个问题

- thread cache：每个线程独享一个thread cache，可以简单的理解为thread cache是每个线程的小内存池，由于被线程独享，所以不需要加锁访问
- central cache：当thread cache无可用内存时，需要向central cache索取内存。当thread cache闲置内存过多时，需要向central cache归还内存，以实现每个thread cache的均衡。由于central cache是多线程共享的，所以需要加锁访问
- page cache：当central无可用内存时，需要从page cache索取内存，page cache会通过系统调用获取堆资源

## thread cache
thread cache是一个线程独享的内存池，当线程需要内存资源时，优先向thread cache索取。和文章一开始实现的定长内存池不同，我们只能向定长内存池索取相同大小的内存块，但是线程的情况很复杂，需要向thread cache申请不同大小的内存块，每次申请的内存块大小几乎不可能相同。

因此thread cache是不定长的，其自由链表也不再是简单的单链表，而是存储单链表的**哈希桶**。

至于thread cache的操作，主要有两个：\_allocate和\_deallocate。

对于\_allocate，我们假设线程能够申请的最大内存块大小为256kB，以8B作为桶的基本单位，每个桶存储的内存块大小为8B，16B，24B，...，256kB，但是这样的自由链表未免有些笨重，竟然有32k个桶，显然是不合理的。所以我们需要设置对齐规则，使桶的数量尽可能的合理：
```cpp
// [1, 128]                         8B对齐           _free_lists[0, 16)
// [128 + 1, 1024]                  16B对齐          _free_lists[16, 72)
// [1024 + 1, 8 * 1024]             128B对齐         _free_lists[72, 128)
// [8 * 1024 + 1, 64 * 1024]        1024对齐         _free_lists[128, 184)
// [64 * 1024 + 1, 256 * 1024]      8 * 1024对齐     _free_lists[184, 208)
```
对于不同的区间设置不同的对齐数，若线程申请的内存块大小不是对齐数的整数倍，那么我们应该向上对齐，多分配给线程一些空间，就算线程压根不会使用，但这是为了能够找到内存块对应的桶。根据上面的对齐规则，thread cache的自由链表只有208个桶，同时线程能够申请的最大内存块大小为256kB，并且在这种程度下，空间的浪费只有10%左右，这是相当不错的了。

刚才提到了空间浪费，这是由向上取整导致的问题，它还有另一个名字：内碎片。通常我们说的内存碎片是指外碎片，外碎片存在于两个内存块之间，由于其空间较小，通常不会也无法被我们使用，因为我们需要连续的大块空间。所以外碎片具有不连续，可利用性低的特点，如果内存中存在大量外碎片，系统的性能将受到极大的影响。但是向上取整导致的内碎片是无法避免的，这是为了追求极致的效率而付出的代价。然而，用可控的内碎片减少不可控的外碎片的产生，也是可以接受的。

综上，要实现thread cache的allocate就要实现两个主要操作：向上取整和桶号映射。但在此之前我们需要对自由链表的子结构进行封装：
```cpp
// 桶的最多数量与内存块的最大字节数
static const size_t NFREELIST = 208;
static const size_t MAX_SIZE = 256 * 1024;

// 提取出obj的指针字段，以引用的形式返回
static void*& next(void* obj)
{
	return *(void**)obj;
}

// 自由链表的子结构：单链表
class FreeList
{
private:
	void* _head = nullptr;
public:
	void _push_front(void* obj);
	void* _pop_front();
	bool _empty();
};

bool FreeList::_empty()
{
	return _head == nullptr;
}

void FreeList::_push_front(void* obj)
{
	if (obj)
	{
		next(obj) = _head;
		_head = (void*)obj;
	}
}

void* FreeList::_pop_front()
{
	if (_head != nullptr)
	{
		void* ret = _head;
		_head = next(_head);
		return ret;
	}

	return nullptr;
}
```
刚才说过thread cache的自由链表的本质是一个哈希桶，而我们封装的这个FreeList子结构就是其中的桶结构。这个结构对外暴露三个接口：判空、元素的插入与删除。

然后是封装一个类SizeClass，以实现向上取整与桶号映射，这个类对外暴露两个接口，分别是：
- 字节数的向上取整
- 字节数和桶号映射

```cpp
class SizeClass
{
private:
	static inline size_t __round_up(size_t bytes, size_t align_num) { return (bytes + (align_num - 1)) & ~(align_num - 1); }
	static inline size_t __get_index(size_t bytes, size_t align) { return ((bytes + (1 << align) - 1) >> align) - 1; }
public:
	static inline size_t _round_up(size_t bytes);
	static inline size_t _get_index(size_t bytes);
};

size_t SizeClass::_round_up(size_t bytes)
{
	int ret = 0;

	if (bytes > 0)
	{
		if (bytes <= 128)
			ret = __round_up(bytes, 8);
		else if (bytes <= 1024)
			ret = __round_up(bytes, 16);
		else if (bytes <= 8 * 1204)
			ret = __round_up(bytes, 128);
		else if (bytes <= 64 * 1024)
			ret = __round_up(bytes, 1024);
		else if (bytes <= 256 * 1024)
			ret = __round_up(bytes, 8 * 1024);
		else
			std::cerr << "__round_up::bytes illegal" << std::endl;
	}
	return ret;
}

size_t SizeClass::_get_index(size_t bytes)
{
	int ret = 0;
	static int bucket_count[4] = { 16, 72, 128, 184 };
	if (bytes > 0)
	{
		if (bytes <= 128)
			ret = __get_index(bytes, 3);
		else if (bytes <= 1024)
			ret = __get_index(bytes - 128, 4) + bucket_count[0];
		else if (bytes <= 8 * 1204)
			ret = __get_index(bytes - 1024, 7) + bucket_count[1];
		else if (bytes <= 64 * 1024)
			ret = __get_index(bytes - 8 * 1024, 10) + bucket_count[2];
		else if (bytes <= 256 * 1024)
			ret = __get_index(bytes - 64 * 1024, 13) + bucket_count[3];
		else
			std::cerr << "__get_index::bytes illegal" << std::endl;
	}
	return ret;
}

```
以上两个类都定义在Common.hpp中。其中，向上取整和桶号映射使用了位运算，这样做可以让算法变得简洁与高效，这也是经常会用到的两个位运算操作。

回到thread cache的\_allocate：
- 我们需要将线程索取的内存块大小映射为桶号
- 根据桶号在哈希桶中找到这个桶
- 判断这个桶是否为空
  - 如果为空，那么thread cache就要向central cache索取内存
  - 如果不为空，那么thread cache就从桶中取出一块内存块（*FreeList的头删操作*）

至于thread cache的deallocate：
- 我们也需要将线程索取的内存块大小映射为桶号
- 根据桶号在哈希桶中找到这个桶
- 将内存块放回桶中（*FreeList的头插操作*）

ThreadCache的实现：
```cpp
#include "Common.hpp"

class ThreadCache
{
private:
	FreeList _free_lists[NFREELIST];
public:
	// 内存块的分配与归还
	void* _allocate(size_t bytes);
	void _deallocate(void* obj, size_t bytes);
};

// TLS
#ifdef _WIN32
static _declspec(thread) ThreadCache* pTLSThreadCache = nullptr;
#else
// 其他系统
#endif

void* ThreadCache::_allocate(size_t bytes)
{
	void* obj = nullptr;
	if (bytes <= MAX_SIZE)
	{
		size_t need_bytes = SizeClass::_round_up(bytes);
		size_t bucket_index = SizeClass::_get_index(bytes);
		if (_free_lists[bucket_index]._empty())
		{
			// 内存不足，向CentralCache索取内存
		}
		else
		{
			obj = _free_lists[bucket_index]._pop_front();
		}
	}

	return obj;
}

void ThreadCache::_deallocate(void* obj, size_t bytes)
{
	if (obj && bytes <= MAX_SIZE)
	{
		size_t bucket_index = SizeClass::_get_index(bytes);
		_free_lists[bucket_index]._push_front(obj);
	}
}
```
其中使用到了：TLS，thread local storage，线程本地存储。因为每个线程需要独享thread cache，所以每个线程都需要有一个自己的ThreadCache对象。我们用一个指针指向ThreadCache对象，并且这个指针是线程独享的，当线程需要申请内存时，首先检查这个指针是否为空，若为空则需要new一个ThreadCache对象并保存其指针，然后通过这个指针调用ThreadCache的\_allocate和\_deallocate。

所以我们再封装两个函数，这两个函数类似于malloc和free。对外只暴露这两个函数，它们会调用thread cache的\_allocate和\_deallocate，但在那之前会先获取TLS对象，也就是ThreadCache对象的指针，通过该指针调用ThreadCache的两个成员函数。
```cpp
// ConcurrentAlloc.hpp
#pragma once

#include "Common.hpp"
#include "ThreadCache.hpp"

static void* tc_allocate(size_t bytes)
{
	if (pTLSThreadCache == nullptr)
		pTLSThreadCache = new ThreadCache;
	
	return pTLSThreadCache->_allocate(bytes);
}

static void tc_deallocate(void* obj, size_t bytes)
{
	if (pTLSThreadCache)
		pTLSThreadCache->_deallocate(obj, bytes);
}
```
这样的话，对于线程的动态资源获取与释放的操作，只要调用tc_allocate和tc_deallocate就行了。
***
对于thread cache的\_allocate实现中，还有一个操作没有实现：某一链表中没有内存块时，ThreadCache应该向CentralCache索取内存，要实现这个操作就要先实现central cache的整体结构。

## central cache
对于central cache，我们需要完成其节点Span，桶（*存储Span的双向链表SpanList*），以及哈希桶的实现。

对于Span：
- Span存储连续的内存页，需要记录起始页号以及拥有的页的数量
- 其次它是一个节点，需要有prev和next两个指针
- 再者，因为Span跨越了多个内存页，而一个内存页的大小通常是4kB，因此central cache需要切分内存页，得到合适的连续的内存块，再分配给thread cache。所以Span需要有一个单链表，该链表为FreeList结构，存储了切分好的多个内存块

central cache将Span保存的内存页切分成多个内存块存储在FreeList结构中，因此Span需要存储FreeList结构，将这些内存块悬挂到FreeList结构下。当thread cache需要内存时，central cache只要头删FreeList，将合理数量的内存块返回即可。所以Span需要一个指向FreeList的指针，来表示被切分好的多个内存块。


对于SpanList：Span只是一个节点，我们需要用它来构成双链表SpanList。central cache需要包含208个SpanList（*和thread cache的FreeList数量一样*），可以把这些双链表理解为哈希桶。这些双链表在哈希桶中的映射规则和thread cache一样，当thread cache的某一桶号下的FreeList没有内存块可用时，我们只需要到central cache的相同桶号下的SpanList中，获取一个可用的Span再访问其FreeList结构，将里面的内存块给thread cache即可。

总结一下：
- Span作为双链表的节点，存储了多个连续的页的信息，每个Span都维护了多块内存块
- SpanList是将Span作为节点的双链表，悬挂了多个Span
- central cache的本质是由208个SpanList构成的一个哈希桶，桶的映射规则与thread cache是对应的

先实现Span结构：
```cpp
// 类似于单链表中的节点
// 其跨越/存储了多个内存页
struct Span
{
	Span* _prev = nullptr;
	Span* _next = nullptr;

	// 存储的内存页其实id与数量
	page_t _id = 0;
	size_t _n = 0;

	// Span维护的多个内存块
	void* _free_list = nullptr;
};
```
因为我不想写构造函数，所以给每个参数带上了缺省值。

再来实现SpanList结构，这是一个带头节点的双链表，对外提供insert和erase操作：
```cpp
// 由Span作为节点的双向链表
// SpanList是CenctralCache的一个桶
class SpanList
{
public:
	SpanList();
	~SpanList();
	// 在pos前插入节点
	void _insert(Span* obj, Span* pos);
	// 删除pos节点
	void _erase(Span* pos);
	// 为什么需要一把锁，这个等等就解释
	std::mutex _mtx;
private:
	Span* _head;
};

SpanList::SpanList()
{
	_head = new Span;
	_head->_prev = _head;
	_head->_next = _head;
}

SpanList::~SpanList()
{
	delete _head;
}

void SpanList::_insert(Span* obj, Span* pos)
{
	Span* prev = pos->_prev;
	
	prev->_next = obj;
	obj->_prev = prev;

	pos->_prev = obj;
	obj->_next = pos;
}

void SpanList::_erase(Span* pos)
{
	Span* prev = pos->_prev;
	Span* next = pos->_next;

	prev->_next = next;
	next->_prev = prev;
}
```

这是很简单的数据结构的操作，没啥好说的。

最后是CentralCache的单例：因为central cache只能也只需要有一个，所以被设计成单例。又因为多个thread cache可以同时访问central cache，所以还需要对central cache加锁。但是考虑到加锁粒度的问题：由于thread cache的哈希桶映射规则和central cache一样，因此thread cache向central cache索取内存块时，只会访问其中一个SpanList。也就是说，获取内存块时，只需要对SpanList加锁，而不用对整个哈希桶加锁。所以SpanList还需要拥有一把锁，当thread cache访问SpanList前，需要先获取这把锁。

central cache的结构：
```cpp
#include "Common.hpp"

class CentralCache
{
private:
	SpanList _span_lists[MAX_SIZE];
public:
	static CentralCache* _get_instance() { return &_instance; }
private:
	CentralCache() {}
	CentralCache(const CentralCache& x) = delete;
	static CentralCache _instance;
};

CentralCache CentralCache::_instance;
```
这个单例没有必要设计成懒汉，为了程序的简单，这里就设计成饿汉。
***
聊完了central cache的具体结构，回到最开始要解决的问题：实现某一链表中没有内存块时，ThreadCache向CentralCache索取内存的操作。

首先，由于线程池中可申请的内存块大小极值相差过大，我们需要设计一个慢启动算法（*类似拥塞控制中的慢启动*）使thread cache每次索取的内存块数量是合理的。

为thread cache的FreeList结构添加一个变量\_fetch\_count，该变量存储了某一FreeList因为内存块不足而向central cache索取的次数，但这个数一开始是1不是0。每次索取内存块后，该数都会自增。修改后的FreeList结构，添加一个成员变量：
```cpp
// 自由链表的子结构：单链表
class FreeList
{
private:
	void* _head = nullptr;
	size_t _fetch_count = 1;
public:
	void _push(void* obj);
	void* _pop();
	bool _empty();
	// 关于_fetch_count的操作
	inline size_t _get_fetch_count() { return _fetch_count; }
	inline void _add_fetch_count() { ++_fetch_count; }
};
```
然后再设计一个算法：对于小的内存块，FreeList索取得多一些。对于大的内存块，FreeList索取得少一些，总体上索取的内存大小是差不多的。计算过程很简单：将内存块大小的最大值（*256kB*） / FreeList对应的内存块大小（*STL的空间配置器中，由于可申请的最大内存块大小为128B，最小为8B，两者相差不大。因此直接默认给20块内存块，但是在我们的内存池中，可申请的最大内存块大小为256kB，对于不同大小的内存块，肯定不能统一对待，多给几块256kB内存块可不是开玩笑的*）。但是该算式得到的结果在极端情况下，对于小内存块来说很大，对于大内存块来说很小，所以这里需要控制一下上下限：
```cpp
size_t SizeClass::_adapt_nums(size_t rounded_bytes)
{
	size_t nums = MAX_SIZE / rounded_bytes;

	if (nums < 2) nums = 2;
	else if (nums > 512) nums = 512;

	return nums;
}
```
拉低上限，提高下限，将结果控制在一个合理的范围。

回到一开始说的慢启动：我们无法保证thread cache可以使用完得到的内存块，可能得到了10块内存块，但thread cache只使用了1块。我们可以先给thread cache分配少量的内存块，如果thread cache还要内存块，我们再给它多一些。所以central cache给thread cache的内存块数量要从\_fetch\_count和\_adapt\_nums函数返回的结果中取最小值。一开始thread cache只能获取一块内存块，之后获取的内存块数量将线性增长。直到\_fetch\_count的值超过了\_adapt\_nums返回的结果，之后thread cache可以得到的内存块数量就等于\_adapt\_nums返回的结果。

我们将central cache分配内存块给thread cache的行为封装成函数\_fetch\_blocks，该函数将获取对应SpanList下的可用Span，并将其维护的内存块尽可能多的返回：
```cpp	
// 将Span维护的内存块返回，以两个指针指向内存块的首尾
void CentralCache::_fetch_blocks(void*& begin, void*& end, size_t& njobs, size_t block_size)
{
	size_t bucket_index = SizeClass::_get_index(block_size);
	
	_span_lists[bucket_index]._mtx.lock();
	// TODO
	Span* span = TODO();
	if (span)
	{
		begin = span->_free_list;
		end = begin;

		int real_jobs = 1;
		// 找到自由链表的尾，并且修改njobs的值
		for (size_t i = 0; next(end) && i < njobs - 1; ++i)
		{
			end = next(end);
			++real_jobs;
		}
		njobs = real_jobs;
		span->_used_count += njobs;

		// 将begin和end间的内存块从Span中删除
		span->_free_list = next(end);
		next(end) = nullptr;
	}
	_span_lists[bucket_index]._mtx.unlock();
}
```
由于需要使thread cache互斥访问的SpanList，所以需要加锁。对于TODO函数：该函数会获取SpanList中的一个Span，这个后续再来实现。其中需要注意的是：begin和end分别指向多块内存块中的第一块与最后一块，一开始end等于begin，所以至少能返回一块，即real_jobs的值为1。之后的循环结束条件不是i < njobs而是i < njobs - 1。

对于\_fetch\_blocks这个接口：该接口会修改begin和end的值，表示其获取的内存块区间，同时也会修改njobs的值表示真正获取到的内存块数量。

对于我们要实现的接口：某一链表中没有内存块时，ThreadCache向CentralCache索取内存的操作。central cache提供给thread cache的接口已经实现，现在要实现thread cache的接口：FreeList需要将从\_fetch\_blocks得到的多个内存块中的一块返回，并将剩下的悬挂到自己的FreeList中。该接口的实现：
```cpp
void* ThreadCache::_fetch_from_central_cache(size_t bucket_index, size_t size)
{
	// 慢启动，取两者的较小值
	size_t adapt_num = SizeClass::_adapt_nums(size);
	size_t fetch_count = _free_lists[bucket_index]._get_fetch_count();
	size_t njobs = fetch_count < adapt_num ? fetch_count : adapt_num;
	// 自增_fetch_count
	if (njobs == fetch_count)
		_free_lists[bucket_index]._add_fetch_count();

	void* begin = nullptr;
	void* end = nullptr;
	CentralCache::_get_instance()->_fetch_blocks(begin, end, njobs, size);

	if (njobs > 1)
		_free_lists[bucket_index]._range_push(next(begin), end);
	return begin;
}
```
可以看到，只有最后得到的值和\_fetch\_count相同时，该变量才会自增，所以准确的说这个变量表示的不是该链表申请内存块的次数，因为当该变量的值超过“adapt_num：一次最大能申请的内存块数量”后，自增就会停止。

其中涉及到“将剩下的内存块悬挂到自己的FreeList中”的操作，这里再为FreeList封装一个操作以实现将一个区间中的内存块头插到链表中的操作：
```cpp
void FreeList::_range_push(void* begin, void* end)
{
	if (begin && end)
	{
		next(end) = _head;
		_head = begin;
	}
}
```
至此，thread cache的接口基本实现完成，以下到目前为止它的所有实现：
```cpp
#pragma once

#include "Common.hpp"
#include "CentralCache.hpp"
class ThreadCache
{
private:
	FreeList _free_lists[NFREELIST];
	// 内存不足时，向CentralCache索取
	void* _fetch_from_central_cache(size_t bucket_index, size_t size);
public:
	// 内存块的分配与归还
	void* _allocate(size_t bytes);
	void _deallocate(void* obj, size_t bytes);
};

// TLS
#ifdef _WIN32
static _declspec(thread) ThreadCache* pTLSThreadCache = nullptr;
#else
// 其他系统
#endif

void* ThreadCache::_allocate(size_t bytes)
{
	void* obj = nullptr;
	if (bytes <= MAX_SIZE)
	{
		size_t need_bytes = SizeClass::_round_up(bytes);
		size_t bucket_index = SizeClass::_get_index(bytes);
		if (_free_lists[bucket_index]._empty())
		{
			obj = _fetch_from_central_cache(bucket_index, need_bytes);
		}
		else
		{
			obj = _free_lists[bucket_index]._pop_front();
		}
	}

	return obj;
}

void ThreadCache::_deallocate(void* obj, size_t bytes)
{
	if (obj && bytes <= MAX_SIZE)
	{
		size_t bucket_index = SizeClass::_get_index(bytes);
		_free_lists[bucket_index]._push_front(obj);
	}
}

void* ThreadCache::_fetch_from_central_cache(size_t bucket_index, size_t size)
{
	// 慢启动，取两者的较小值
	size_t adapt_num = SizeClass::_adapt_nums(size);
	size_t fetch_count = _free_lists[bucket_index]._get_fetch_count();
	size_t njobs = fetch_count < adapt_num ? fetch_count : adapt_num;
	// 自增_fetch_count
	if (njobs == fetch_count)
		_free_lists[bucket_index]._add_fetch_count();

	void* begin = nullptr;
	void* end = nullptr;
	CentralCache::_get_instance()->_fetch_blocks(begin, end, njobs, size);

	if (njobs > 1)
		_free_lists[bucket_index]._range_push(next(begin), end);
	return begin;
}
```
***
而刚才实现的接口\_fetch\_blocks中还有一个接口没有实现：从SpanList中获取Span的操作。先理解SpanList与桶号的映射关系：central cache的某一SpanList下可能悬挂了Span节点，这些节点是连续的内存页，Span被切分成内存块后，内存块的大小和桶号具有映射关系。又因为central cache和thread cache的哈希桶映射规则相同，所以当thread cache某一桶下的内存块不足时，只需找central cache相同桶号下的Span要即可。用数据结构来理解，当thread cache的某一FreeList内存块数量不足，就要去central cache对应SpanLlist下索取可用Span，可用Span的内存块就是thread cache所需要的。

当SpanList下没有一个可用Span时，就要向page cache申请一个span（*这个接口等到实现完page cache的大体结构后再来实现*），而SpanList得到的新的span需要被切分成对应大小的内存块后才可用使用。所以这个接口还负责了span向内存块的切分：
```cpp
Span* CentralCache::_get_span(SpanList& span_list, size_t block_size)
{
	Span* cur = span_list._begin();
	Span* end = span_list._end();

	// 遍历SpanList，查找是否有可用的Span
	while (cur != end)
	{
		if (cur->_free_list)
		{
			// 注意不要erase(cur)
			return cur;
		}
		cur = cur->_next;
	}

	// 先解桶锁，使后续的thread cache可用访问（可能归还，可能索取）
	span_list._mtx.unlock();
	// TODO::无可用Span，向page cache求助
	Span* ret_span = TODO();

	if (ret_span)
	{
		// 获取到span后，切分span
		void* start = (void*)(ret_span->_id << PAGE_SHIFT);
		void* finish = (void*)((ret_span->_id + ret_span->_n) << PAGE_SHIFT);

		ret_span->_free_list = start;

		page_t pending = (ret_span->_id << PAGE_SHIFT) + block_size;

		while ((void*)pending != finish)
		{
			next(start) = (void*)pending;
			pending += block_size;
			start = next(start);
		}
		next(start) = nullptr;
	}
	// 此时要返回到_fetch_blocks函数，需要加锁
	span_list._mtx.lock();
	return ret_span;
}
```
\_get\_span首先会查找SpanList下是否有可用span，若一个可用span都没有则会向page cache求助，得到新的span，将其切分后返回。

关于切分算法的实现：
- 先定义两个指向头尾的void\*指针，此时span的内存地址为\[start, finish)
- 然后定义一个整数pending，切分内存块时，该数表示下一个内存块的地址
- 将start作为\_free\_list的值，然后开始将start和pending进行链接
- pending每次增加一个内存块的大小，当pending的值和finish一样，说明pending此时位于span的尾端 + 1字节的位置，此时不能再链接
- 最后，将span的最后一块内存块的下一指针置空，表示链表的结束

其中要注意的是：void\*的加减法和普通整数的加减法不同，所以pending使用page_t类型定义。

至此，将目前为止实现的CentralCache展示出来：
```cpp
#pragma once

#include "Common.hpp"
#include "PageCache.hpp"

class CentralCache
{
private:
	SpanList _span_lists[MAX_SIZE];
public:
	static CentralCache* _get_instance() { return &_instance; }
	// 将Span维护的内存块返回，以两个指针指向内存块的首尾
	void _fetch_blocks(void*& begin, void*& end, size_t& njobs, size_t block_size);
	// 获取一个Span
	Span* _get_span(SpanList& span_list, size_t block_size);
private:
	CentralCache() {}
	CentralCache(const CentralCache& x) = delete;
	static CentralCache _instance;
};

CentralCache CentralCache::_instance;

void CentralCache::_fetch_blocks(void*& begin, void*& end, size_t& njobs, size_t block_size)
{
	size_t bucket_index = SizeClass::_get_index(block_size);

	_span_lists[bucket_index]._mtx.lock();
	Span* span = _get_span(_span_lists[bucket_index], block_size);
	if (span && span->_free_list)
	{
		begin = span->_free_list;
		end = begin;

		int real_jobs = 1;
		// 找到自由链表的尾，并且修改njobs的值
		for (size_t i = 0; next(end) && i < njobs - 1; ++i)
		{
			end = next(end);
			++real_jobs;
		}
		njobs = real_jobs;
		span->_used_count += njobs;

		// 将begin和end间的内存块从Span中删除
		span->_free_list = next(end);
		next(end) = nullptr;
	}
	_span_lists[bucket_index]._mtx.unlock();
}

Span* CentralCache::_get_span(SpanList& span_list, size_t block_size)
{
	Span* cur = span_list._begin();
	Span* end = span_list._end();

	// 遍历SpanList，查找是否有可用的Span
	while (cur != end)
	{
		if (cur->_free_list)
		{
			// 注意不要erase(cur)
			return cur;
		}
		cur = cur->_next;
	}

	// 先解桶锁，使后续的thread cache可用访问（可能归还，可能索取）
	span_list._mtx.unlock();
	// TODO::无可用Span，向page cache求助
	Span* ret_span = TODO();

	if (ret_span)
	{
		// 获取到span后，切分span
		void* start = (void*)(ret_span->_id << PAGE_SHIFT);
		void* finish = (void*)((ret_span->_id + ret_span->_n) << PAGE_SHIFT);

		ret_span->_free_list = start;

		page_t pending = (ret_span->_id << PAGE_SHIFT) + block_size;

		while ((void*)pending != finish)
		{
			next(start) = (void*)pending;
			pending += block_size;
			start = next(start);
		}
		next(start) = nullptr;
	}
	// 此时要返回到_fetch_blocks函数，需要加锁
	span_list._mtx.lock();
	return ret_span;
}
```
和thread cache一样，central cache也有一个等待实现的接口，因为这个接口也涉及到其他结构的实现，所以我们需要先实现这个结构。

## page cache
page cache用来做什么？当central cache无内存可用时，就会向page cache索取内存。对于thread cache，当其需要内存块时，central cache会将内存块返回，而central cache向page cache申请的却是span，所以central cache自己承担了将span切分为内存块的工作。这个接口刚刚也实现过了，可以看出central cache是一个承上启下的结构。

关于page cache，其结构也是一个哈希桶，桶结构也是SpanList，但是映射规则和central不一样，他有129个桶，第1桶不使用，从第二个桶开始使用，也就是从下标为1的位置使用数组。每个下标对应的是span的页数，比如下标为5的位置，该位置的SpanList悬挂的都是大小为5页的span。并且page cache也是一个单例，central cache也需要互斥访问它，总之，先搭建个大致的结构：
```cpp
class PageCache
{
private:
	PageCache() {}
	PageCache(const PageCache& x) = delete;
public:
	static PageCache* _get_instance() { return &_instance; }
private:
	static PageCache _instance;
	SpanList _span_lists[NPAGES];
	std::recursive_mutex _rmtx;
};
PageCache PageCache::_instance;
```

当central向page索取span时，需要告知page自己要索取的span的页数，然后page就会找到对应的桶，检查该SpanList下是否有span可用。若没有span，page cache不会直接向堆区申请内存，而是往后查找是否有更大的span可以使用，若找到了就进行切分，若没找到才会向堆区申请内存。
```cpp
Span* PageCache::_fetch_kspan(size_t k)
{
	// 获取PageCache的span时需要加锁
	_mtx.lock();
	if (!_span_lists[k]._empty())
	{
		Span* ret_span = _span_lists[k]._pop_front();
		_mtx.unlock();
		return ret_span;
	}
	else
	{
		for (int i = k + 1; i < NPAGES; ++i)
		{
			// 往后找更大的span，进行切分
			if (!_span_lists[i]._empty())
			{
				// 找到不为空的SpanList，获取第一个Span，该Span的大小为i个page
				Span* old_span = _span_lists[i]._pop_front();
				Span* ret_span = new Span;
				ret_span->_n = k;
				ret_span->_id = old_span->_id;

				old_span->_n -= k;
				old_span->_id += k;
				_span_lists[i]._push_front(old_span);

				_mtx.unlock();
				return ret_span;
			}
		}

		//  没有更大的Span可以用，此时向堆区申请一块128page的空间
		void* ptr = SystemAlloc(128);
		
		Span* max_span = new Span;
		max_span->_id = (page_t)ptr >> PAGE_SHIFT;
		max_span->_n = 128;
		_span_lists[128]._push_front(max_span);

		_mtx.unlock();
		return _fetch_kspan(k);
	}
}
```
关于切分算法实现：向后找到更大的未使用的span后，需要new一个新的span保存切分后得到的合适大小的span。切分需要修改原span的信息，比如起始页号、跨越的页数，然后修改新span的信息。最后将修改后的旧span重新插入到SpanList中，将切好的span返回。

当page cache山穷水尽（*没有更大内存块可用时*），就会调用VirtualAlloc向堆区申请资源（*其他系统的接口不同*），申请到一个128页的span，将其插入到哈希桶中。然后将其切分，一个span被返回，一个span插入到Span List中。

对于申请128页span后的切分操作，我们不用重新写代码实现，本着代码重用的原则，我们可用将128页的span插入，然后递归调用自己。这个递归调用会继续申请k页的span，也就是会将128页的span进行切分，一个span被重新插入，一个span被返回。

所以，到现在page cache的获取span操作基本实现完成。这个内存池的基本框架也已经搭建起来，总结一下：
- 内存池分为三层，从堆区到线程依次是：page cache、central cache、thread cache
- 线程获取和释放内存不再调用malloc和free，而是调用我们封装的接口：tc_allocate和tc_deallocate
- tc_allocate的逻辑已经实现：
  - 调用thread cache的_allocate申请内存，该接口将返回一块内存块，其中会产生内碎片
  - 若thread cache内存块不足，将调用_fetch_from_central_cache向central cache申请内存块
  - central cache作为一个承上启下的结构，其拥有的不是内存块，而是span
  - central cache会通过_fetch_blocks接口将内存块返回给thread cache
  - 当central cache无可用span，将会调用_get_span接口，向page cache申请span，该接口还负责将申请到的span切分成内存块的操作
  - page cache通过_fetch_kspan接口，将一个k页的span返回给cenctral cache
  - 当page cache也无可用span时，会调用系统接口，向系统申请堆区资源

当然了，以上梳理的只是一个大致的流程，其中还有很多细节没有展示。
***
现在基本都写完了，就是差一个调整函数。其中有两个调整函数，一个是告知内存块的大小，得到合适的内存块数量。另一个就是这个，需要通过前一个接口得到合适的内存块数量，然后将内存块数量乘以内存块大小，再右移得到页的数量，如果右移的结果是0，至少也要返回一块，得到合适的页数

***
## 关于内存块的归还
thread cache将闲置的内存块归还给central cache，central cache将span归还给page cache，page cache将小块的span合并成大块的span。归还操作是高并发内存池的精髓，同时这个操作也是为了解决外碎片问题。

首先是thread的内存块归还，当FreeList的长度大于该链表一次能申请的最大数量时需要进行归还操作（*当然，还可以添加更多的条件，使空闲内存块尽可能早的归还*），归还这些内存块给central cache。可以注意到，我们无法很快的得知FreeList的长度，所以这里再为FreeList创建一个变量以保存其长度，并且修改相关push和pop接口，因为进行这些操作会改变链表的长度，以下给出FreeList到目前为止的实现：
```cpp
class FreeList
{
private:
	void* _head = nullptr;
	size_t _fetch_count = 1;
	size_t _length = 0;
public:
	void _push_front(void* obj);
	void* _pop_front();
	bool _empty();
	// 将begin到end之间的内存块，头插到链表中
	void _range_push(void* begin, void* end, size_t count);
	// 将链表头部往后fetch_count块内存块删除，以输出型参数的方式返回区间中的头尾两块内存块
	void _range_pop(void*& begin, void*& end, size_t rounded_bytes);
	// 关于_fetch_count与_length的操作
	inline size_t _get_fetch_count() { return _fetch_count; }
	inline void _add_fetch_count() { ++_fetch_count; }
	inline size_t _get_length() { return _length; }
};

bool FreeList::_empty()
{
	return _head == nullptr;
}

void FreeList::_push_front(void* obj)
{
	if (obj)
	{
		next(obj) = _head;
		_head = (void*)obj;
		_length += 1;
	}
}

void* FreeList::_pop_front()
{
	if (_head != nullptr)
	{
		void* ret = _head;
		_head = next(_head);
		_length -= 1;
		return ret;
	}

	return nullptr;
}

void FreeList::_range_push(void* begin, void* end, size_t count)
{
	if (begin && end)
	{
		next(end) = _head;
		_head = begin;
		_length += count;
	}
}

void FreeList::_range_pop(void*& begin, void*& end, size_t rounded_bytes)
{
	begin = _head;
	end = (void*)((page_t)_head + (_fetch_count - 1) * rounded_bytes);

	_head = next(end);
	next(end) = nullptr;
	_length -= _fetch_count;
}
```

thread cache归还操作的逻辑：调用链表的\_range\_pop接口，把\_fetch\_count个内存块从链表中移除，再将它们还给central cache。而链表的移除接口会返回两个指向内存块区间首尾的指针，我们需要告知central cache这两个指针。

最后是修改thread cache的\_deallocate接口，在最后添加归还操作的判断：
```cpp
void ThreadCache::_deallocate(void* obj, size_t bytes)
{
	if (obj && bytes <= MAX_SIZE)
	{
		size_t bucket_index = SizeClass::_get_index(bytes);
		FreeList& aim_list = _free_lists[bucket_index];
		aim_list._push_front(obj);

		// 链表长度大于一次能获取的最大内存块数量，需要归还
		if (aim_list._get_length() > aim_list._get_fetch_count())
		{
			void* start = nullptr;
			void* finish = nullptr;
			aim_list._range_pop(start, finish, SizeClass::_round_up(bytes));

			// TODO调用central的归还
		}
	}
}
```

接着是central cache从thread cache接收内存块的逻辑：central cache需要知道“内存块区间”的第一个内存块地址（*因为最后一块内存块的下一指针将指向空，所以只需要一个起始地址即可*）和该内存块对应的桶号（*将内存块放到哪里*）。对于每一块内存块，central cache需要将其归还到正确的span中，所以这里要设计算法实现内存块与span之间的映射，这是后话了，先跳过。当central cache得知了内存块位于的span，就会将其插入到span的FreeList中，并减少_usecount的值。当_usecount的值为0，说明该span切分后的内存块没有线程使用，此时central cache需要将span归还给page cache：
```cpp
void CentralCache::_return_blocks_to_spans(void* start, size_t bucket_index)
{
	_span_lists[bucket_index]._mtx.lock();
	while (start)
	{
		Span* aim_span = TODO();// 将内存块转换成span的地址
		if (aim_span == nullptr)
		{
			std::cerr << "内存块没有从属的span" << std::endl;
		}
		else
		{
			next(start) = aim_span->_free_list;
			aim_span->_free_list = start;
			--(aim_span->_used_count);

			if (aim_span->_used_count == 0)
			{
				// TODO::向page归还span
				_span_lists[bucket_index]._erase(aim_span);
			}
		}
		start = next(start);
	}
	_span_lists[bucket_index]._mtx.unlock();
}
```
当然了，高并发的场景下，线程需要加锁访问central cache。

接着实现TODO接口“page cache接收central cache归还的span”：这个接口由page cache提供，它需要接收一个span的地址，并尽可能地将该span和未使用的span进行合并，得到一个更大的span。所以这里要设计一个操作：查找与该span相邻的且未使用的span。

- 相邻的span：这个相邻指的是物理内存上的相邻，也就是页号的相邻。而要合并span肯定要知道span的地址，问题就转换成了：推测相邻的页号，将这些页号转换成span的地址，进而判断这些span是否在使用。
- 是否在使用：为Span添加一个bool类型的成员变量
- 页号和地址的转换：使用映射表unordered_map将页号和span\*进行映射，只要page cache申请了或者切分了span，就要为这些span建立页号和地址的映射关系

所以我们只要用页号在映射表中进行查找，就能推断出哪些页号没有被申请。只要通过页号找到其span，通过Span的bool类型变量就能推断出该span是否被central cache使用。

添加映射表以及相关操作：
```cpp
class PageCache
{
private:
	PageCache() {}
	PageCache(const PageCache& x) = delete;
public:
	static PageCache* _get_instance() { return &_instance; }
	// 获取一个跨越k页的span
	Span* _fetch_kspan(size_t k);
	// 接收来自central归还的span
	void _return_span_to_spans(Span* span);
	// 根据obj判断该内存块所属的span
	Span* _map_obj_to_span(void* obj);
	// 将k个页和span建立映射关系
	void _map_span(page_t page_id, size_t k, Span* span);
private:
	static PageCache _instance;
	SpanList _span_lists[NPAGES];
	std::recursive_mutex _rmtx;;
	std::unordered_map<page_t, Span*> _pageid_to_span;
};
```
其中
- \_pageid\_to\_span就是所谓的映射表
- \_map\_obj\_to\_span可以将内存块的地址转换成span的地址
- \_map\_span可以在映射表中为从“page\_id往后的k个页”与“span”建立映射关系

然后为Span添加成员变量，其初始值为flase，表示该span未被使用：
```cpp
struct Span
{
	Span* _prev = nullptr;
	Span* _next = nullptr;

	// 存储的内存页其实id与数量 
	page_t _id = 0;
	size_t _n = 0;

	// Span切好或合并后的单链表，以及分配出去的内存块数量
	void* _free_list = nullptr;
	size_t _used_count = 0;

	// 是否被使用
	bool _is_used = false;
};
```
接着说明_return_span_to_spans的逻辑：
- 往前合并：判断该span的前一页是否被申请且未被使用，同时合并后的span大小不超过128页，若满足以上条件，合并两个span。然后继续往前合并
- 往后合并：同样的也是满足以上三个条件就可以进行后续的合并
- 最后将合并好的span插入到SpanLlist中

需要注意的是，每次的合并都要维护映射表：
```cpp
void PageCache::_return_span_to_spans(Span* span)
{
	// 合并的过程需要加锁
	std::unique_lock<std::recursive_mutex> guard(_rmtx);

	// 往前合并
	while (true)
	{
		page_t prev_page_id = span->_id - 1;

		auto it = _pageid_to_span.find(prev_page_id);
		if (it == _pageid_to_span.end()) break;

		Span* prev_span = it->second;
		
		if (prev_span->_is_used == true) break;
		if (prev_span->_n + span->_n > 128) break;

		span->_id = prev_span->_id;
		span->_n += prev_span->_n;

		// 删除被合并的span的映射关系
		_erase_map(prev_page_id);
		_erase_map(prev_page_id + prev_span->_n - 1);
		// 删除SpanList中被合并的span
		_span_lists[prev_span->_n]._erase(prev_span);
		delete prev_span;
	}

	// 往后合并
	while (true)
	{
		page_t next_page_id = span->_id + span->_n;

		auto it = _pageid_to_span.find(next_page_id);
		if (it == _pageid_to_span.end()) break;

		Span* next_span = it->second;

		if (next_span->_is_used == true) break;
		if (next_span->_n + span->_n > 128) break;

		span->_n += next_span->_n;
		
		// 删除被合并的span的映射关系
		_erase_map(next_page_id);
		_erase_map(next_page_id + next_span->_n - 1);
		
		// 删除被合并的span
		_span_lists[next_span->_n]._erase(next_span);
		delete next_span;
	}

	span->_is_used = false;
	_pageid_to_span[span->_id] = span;
	_pageid_to_span[span->_id + span->_n - 1] = span;
	_span_lists[span->_n]._push_front(span);
}
```

关于映射表的创建：什么时候要建立映射关系？当page的哈希桶申请128页的span时，需要建立映射吗？如果建立了映射，就是要循环128次，将128个页和span建立关系。但是这128页的span后面肯定会被切分，因为线程一次性能申请的最大内存块大小为256kB，远远不及128页。现在的问题是：有必要建立128个映射关系吗？因为page cache需要加锁访问，在高并发场景下，加锁访问临界资源的时间越少越好，虽然128个比较少，但还是需要考虑一下的。

当page cache把span分配给central cache时，thread cache极有可能使用该span下的内存块。当thread cache归还这些内存块时，central cache需要通过内存块地址找到span的地址，也就是调用page cache的\_map\_obj\_to\_span的接口。此时该span跨越的所有页都要和span建立映射关系，否则内存块将找不到对应span。

那么未被分配的span需要将所有页和span建立映射关系吗？这就要思考未被分配的span在page cache中可能会进行什么操作
- 首先，\_map\_obj\_to\_span肯定不会有问题，因为这些span还在page cache中，内存块未被切分，肯定不会出现内存块找不到对应span的情况
- 其次，\_return\_span\_to\_spans会根据span的页号合并span，当然了，被合并的span肯定都是在page cache中的
- 最后，\_fetch\_kspan会将span分配给central cahe，此时span的所有页都要和span建立映射关系

综上，未被分配的span只要将首尾页号与span建立映射关系，被分配的span需要将所有页号与span建立映射关系。

***
修改申请和释放接口，针对256kB以上内存的申请进行修改

修改释放接口：使之不接收用户想要释放的内存块大小，所以这个接口只知道内存块的起始地址。而问题的本质在于：是否要贯穿我们设计的三层结构申请内存？一般情况下，一页的大小为4/8kB，申请小于等于256kB的内存块将使用三层结构。但我们设计的page cache的哈希桶能存储128页的span，256kB最多只使用了哈希桶的前64页，还有一半的哈希桶没有被使用。为尽可能减少内存碎片的问题，我们要尽量使用page cache，充分利用其合并span的接口。

通过地址我们只能获取其span，而Span结构没有表示内存块大小的变量，所以这里再添加一个成员变量表示内存块的大小。多数情况下，这个变量的值由切分内存块的central cache的_get_span接口修改，page cache不修改。只有申请内存块大于256kB时，page cache才会修改其值，因为这些内存块只有page cache管理。

但是每次释放内存块都要加锁访问page cache的映射表，很明显，这样会加剧线程间的竞争。

针对内存块大小，我们可以分成三种情况：
- 小于等于256kB的内存块，申请操作将贯穿三层结构
- 大于256kB但小于等于128页的内存块，申请操作只涉及page cahe，不涉及其他两层结构
- 大于128页的内存块，直接调用系统接口，虽然申请操作也需要涉及page cache，但是不会将获得的内存块插入哈希桶，只是使用page cache的映射表，方便释放内存时得知内存块大小

因此，我们需要对tc_allocate和tc_deallocate进行再设计：
```cpp
static void* tc_allocate(size_t need_bytes)
{
	if (pTLSThreadCache == nullptr)
		pTLSThreadCache = new ThreadCache;

	// for debug
	std::cout << std::this_thread::get_id() << ":" << pTLSThreadCache << std::endl;

	void* obj = nullptr;
	if (need_bytes > ((NPAGES - 1) << PAGE_SHIFT))
	{
		obj = system_alloc(SizeClass::_round_up(need_bytes));
	}
	else if (need_bytes > MAX_SIZE)
	{
		size_t block_size = SizeClass::_round_up(need_bytes);
		Span* span = PageCache::_get_instance()->_fetch_kspan(block_size >> PAGE_SHIFT);
		span->_block_size = block_size;
		span->_is_used = true;
		obj = (void*)(span->_page_id << PAGE_SHIFT);
	}
	else
	{
		obj = pTLSThreadCache->_allocate(need_bytes);
	}
	
	return obj;
}

static void tc_deallocate(void* obj)
{
	if (pTLSThreadCache)
	{
		Span* span = PageCache::_get_instance()->_map_obj_to_span(obj);
		size_t block_size = span->_block_size;
		if (block_size > (NPAGES - 1) << PAGE_SHIFT)
		{
			system_dealloc(obj, block_size);
		}
		else if (block_size > MAX_SIZE)
		{
			PageCache::_get_instance()->_return_span_to_spans(span);
		}
		else
		{
			pTLSThreadCache->_deallocate(obj, block_size);
		}
	}
}
```

其实深入的思考下，关于大块内存块的申请与释放，重点不在过程怎么设计，重点在：给你一个地址，你要怎么知道以该地址为其实地址的内存块的大小？这样一想，大于128页的内存申请就不能和page cache分离了，因为如果这样，内存块的大小便无从得知。或者说有更好的设计？这个地方以后可以看看源码学习一下。

其次，修改page cache的\_fetch\_kspan接口：
- 当申请的span页数超过128页时，直接调用系统接口，获取内存后向映射表添加映射关系（*只添加起始页和span的映射关系）*，最后返回
- 当申请的span大小超过256kB时，操作需要贯穿page cache的哈希桶，但是只需要添加起始页和span的映射关系
- 当申请的span大小小于等于256kB时，操作需要贯穿page cache的哈希桶，并添加所有页和span的映射关系

```cpp
Span* PageCache::_fetch_kspan(size_t k)
{
	// 获取PageCache的span时需要加锁
	std::unique_lock<std::recursive_mutex> guard(_rmtx);
	// 申请大于的span大于128页
	if (k > NPAGES - 1)
	{
		void* obj = system_alloc(k);
		Span* ret_span = new Span;
		ret_span->_page_id = (page_t)obj >> PAGE_SHIFT;
		ret_span->_block_size = k << PAGE_SHIFT;
		ret_span->_is_used = true;
		ret_span->_n = k;
		// 添加起始页的映射关系
		_map_span(ret_span->_page_id, 1, ret_span);
	}

	if (!_span_lists[k]._empty())
	{
		Span* ret_span = _span_lists[k]._pop_front();
		// 注意区分不同的页
		if (k > MAX_SIZE >> PAGE_SHIFT)
			_map_span(ret_span->_page_id, 1, ret_span);
		else
			_map_span(ret_span->_page_id, k, ret_span);

		ret_span->_is_used = true;
		return ret_span;
	}
	else
	{
		for (int i = k + 1; i < NPAGES; ++i)
		{
			// 往后找更大的span，进行切分
			if (!_span_lists[i]._empty())
			{
				// 找到不为空的SpanList，获取第一个Span，该Span的大小为i个page
				Span* old_span = _span_lists[i]._pop_front();
				Span* ret_span = new Span;
				ret_span->_n = k;
				ret_span->_page_id = old_span->_page_id;

				old_span->_n -= k;
				old_span->_page_id += k;
				_span_lists[i]._push_front(old_span);
				
				// 两span映射关系的维护
				_map_span(old_span->_page_id, 1, old_span);
				if (k > MAX_SIZE >> PAGE_SHIFT)
					_map_span(ret_span->_page_id, 1, ret_span);
				else
					_map_span(ret_span->_page_id, k, ret_span);

				ret_span->_is_used = true;
				return ret_span;
			}
		}

		//  没有更大的Span可以用，此时向堆区申请一块128page的空间
		void* ptr = system_alloc(128);
		// TODO::这里需要对ptr为空的情况进行异常处理
		Span* max_span = new Span;
		max_span->_page_id = (page_t)ptr >> PAGE_SHIFT;
		max_span->_n = 128;
		_span_lists[128]._push_front(max_span);

		// 128页的span映射关系的建立（首尾）
		_map_span(max_span->_page_id, 1, max_span);
		_map_span(max_span->_page_id + 127, 1, max_span);
		return _fetch_kspan(k);
	}
}
```
然后修改\_return_span_to_spans接口：
- 当归还的页数大于128页，调用系统接口，直接将其返回给系统
- 当归还的页数小于等于128页，需要进行合并操作

```cpp
void PageCache::_return_span_to_spans(Span* span)
{
	// 合并的过程需要加锁
	std::unique_lock<std::recursive_mutex> guard(_rmtx);
	if (span->_n > NPAGES - 1)
	{
		system_dealloc((void*)(span->_page_id << PAGE_SHIFT), span->_n << PAGE_SHIFT);
		return;
	}

	// 往前合并
	while (true)
	{
		page_t prev_page_id = span->_page_id - 1;

		auto it = _pageid_to_span.find(prev_page_id);
		if (it == _pageid_to_span.end()) break;

		Span* prev_span = it->second;
		
		if (prev_span->_is_used == true) break;
		if (prev_span->_n + span->_n > 128) break;

		span->_page_id = prev_span->_page_id;
		span->_n += prev_span->_n;

		// 删除被合并的span的映射关系
		_erase_map(prev_page_id);
		_erase_map(prev_page_id + prev_span->_n - 1);
		// 删除SpanList中被合并的span
		_span_lists[prev_span->_n]._erase(prev_span);
		delete prev_span;
	}

	// 往后合并
	while (true)
	{
		page_t next_page_id = span->_page_id + span->_n;

		auto it = _pageid_to_span.find(next_page_id);
		if (it == _pageid_to_span.end()) break;

		Span* next_span = it->second;

		if (next_span->_is_used == true) break;
		if (next_span->_n + span->_n > 128) break;

		span->_n += next_span->_n;
		
		// 删除被合并的span的映射关系
		_erase_map(next_page_id);
		_erase_map(next_page_id + next_span->_n - 1);
		
		// 删除被合并的span
		_span_lists[next_span->_n]._erase(next_span);
		delete next_span;
	}

	span->_is_used = false;
	_map_span(span->_page_id, 1, span);
	_map_span(span->_page_id + span->_n - 1, 1, span);
	_span_lists[span->_n]._push_front(span);
}
```


***
## 待解决的问题
全局函数在多个cpp文件中使用导致的链接冲突，static解决？原理是什么？做完项目再来理解吧
又遇到一个问题：lock_guard的使用，虽然它保证了异常安全，但是想要手动解锁时，却比较麻烦
lock_guard的解锁是个谜
span没有复用FreeList结构，而是用原始的指针保存多块内存块
系统接口需要了解一下，这块可能有bug
unique_lock不能拷贝，但接收其地址后为什么不能调用lock函数？
## 收获
命名很重要，能直观的表达意思的变量与函数名不仅提高了可读性也提高了可维护性
下次做项目时，记得多添加一些打印语句，方便观察选项与调试，也能更好的获得反馈
同上，多用assert，使错误尽可能早的暴露。调试语句应该使用类似日志的那种格式
***
## 遇到的问题：
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230508194114.png)


定位：central cache向page cache申请span后进行切分时的错误

看上去是切分内存块时遇到的错误，然后看了pending和finish，pending超过了finish很多，这里应该是非法访问被检查出来了。

所以while循环的判断条件不要随便的写上!=，当你不清楚循环变量的具体数值，它会怎样增长时，最好写上<号。

问题就是页的大小不是内存块的倍数，所以这里无法使用所有内存作为内存块，会有一小块内存不足而无法使用
***

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230508220214.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230508220118.png)

定位：和上面一样，同样是切分内存块时程序崩溃，看了一下崩溃在对start的解引用，显然是page cache给的span有问题，从start开始往后的访问是非法的

调试看了下，发现span的页数有问题，往前溯源，发现128页的桶中存储了1页的span，考虑可能是SpanList的头删或其他操作有问题，但最终被排除。继续检查切分的代码，最终发现问题在：切分后的span没有插入到对应桶中

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230509224045.png)

这个问题是在观察调试信息时发现的，挺隐蔽的一个错误。目前没什么头绪，这也不会给程序带来太大的影响
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230510145841.png)

span是函数的返回值，当span不存在时，函数才会返回空。
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230510152024.png)

页数为1时，会有问题吗？当我把鼠标放到prev_span->\_n上面时，调试器显示出1的一瞬间，就明白了。

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230510161444.png)

我弄混了prev_page_id和prev_span->\_page_id的含义，span有多个页，prev_page_id可能是span的最后一页，而prev_span->\_page_id却是span的起始页，我将被合并span的最后一页作为合并后span的起始页，这明显是有问题的。这个bug是怎么发现的呢？其实是打印语句调式时，我将prev_page_id作为被合并span的起始页号，通过打印信息的不一致发现的这个问题。如果不是错上加错，不知道这个bug要调到什么时候。
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230511115456.png)

还是这个问题，上次没有完全改好。。。调了很久
***
真的是死锁啊。程序似乎不是死锁，现象是申请释放同一大小内存块两次，会卡在第二次的释放操作上，具体是释放操作的合并span有问题，又是合并。。。

6，release模式调了很久
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230511203207.png)

这也太大了吧，好像懂了，归还的span的used_count没有被清零。还有一种可能，它是size_t，减去值的时候减多了
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230511204631.png)

这里要结束循环。结束个锤子，归还连续内存块的过程中，一个span的内存块全部收回，将其放回给page cache。问题是下一个归还的内存块为什么还能对应到这个被归还的span，这就说明了内存块数量和span的used_count之间的对应有问题。先跳过，明天开始总结项目的过程，先理清楚一遍结构再来找问题，总感觉begin和end要设计成左闭右开的形式，直接是闭区间感觉怪怪的，极可能导致差一错误。
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516154831.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516154758.png)

更新start时会错误，这。。。还是数据结构不够熟练啊

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516154738.png)
这个错误很明显，没有获取到对应桶号下的spanlist
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516160759.png)
thread cache的FreeList存储的内存块一定是连续的吗？很显然不一定是连续的，所以这个迭代器不是随机迭代器，而是单向迭代器，只能老老实实的往后遍历。

还有fetch_count的值一开始是1，然后申请一块内存块就+1，直接减去fetch_count会有问题。
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516163028.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516163400.png)
首先njobs要减1
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516191328.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230516191446.png)
***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519131318.png)
调试多线程时需要的问题：映射表的删除遇到了不存在的span，所以要检查哪些函数调用了删除，或者是添加不到位导致
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519133700.png)

原来还是我的代码存在问题，诶
***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519133914.png)
从内存来看，这应该是非法访问的问题，这么说就是start的来源有问题，即获取的span有问题。
可能是系统调用接口没整明白，晚上看下接口的说明即可，如果不是这个问题，那就再找其他问题。
因为start是根据span的页号确定的，span的页号有问题？第一次切分内存块就越界了，所以是span获取的页有问题
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519184053.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519184752.png)


***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519181132.png)

这个也是非法访问，要插入的位置pos的next为未初始化的地址，pos由begin()接口获取，为head的next。也就是说head的next指向了一个未初始化的区域，猜测是删除时没有维护好链表
***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519190607.png)
***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519212644.png)

![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230519213230.png)

这是切分的实现，这里的i为52，也就是说在52页的桶中，找到了2页的span？因为该桶不为空，很显然，是双链表出了问题。

有没有可能是多线程并发出现了问题？单线程不会切分出0页的span。这里加个打印信息，看看是否是临界资源出了问题

***
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230520095418.png)

若链表有一个节点，那么prev和next都将指向这个节点，这里的指向有问题。
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230520100809.png)

同时竟然会出现一些n为0的页，哦这个页数是减完之后的，还没被删除的span的
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230520104131.png)

出错的链表大概是这样的情况，有节点的next没有正确的指向。此时将无法删除该节点
![image.png](https://raw.githubusercontent.com/ren77281/pigco-image/main/img/20230520104543.png)

然而之前的2号桶也出现了问题，共同点就是next或者prev指针指向了自己，而该节点却不是头节点。先检查链表操作是否有问题，然后检查是否重入了函数

### 补充：系统接口的使用
